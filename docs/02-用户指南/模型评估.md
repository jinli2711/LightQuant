# 模型评估

本章节详细介绍 LightQuant 项目的模型评估方法，包括评估指标定义、评估流程和结果分析。正确的模型评估是判断预测模型性能的基础，也是进行模型选择和优化的依据。

## 评估指标体系

### 分类准确率（ACC）

分类准确率是最直观的评估指标，表示预测正确的样本占总样本的比例。计算公式为：ACC = (预测正确数) / (总样本数)。对于二分类问题，预测正确包括真正例（True Positive）和真负例（True Negative）。准确率在样本平衡时是有效的评估指标，但在样本不平衡时可能产生误导。

在股票预测场景中，由于涨跌样本通常接近平衡，准确率是一个有参考价值的指标。但需要注意，基准准确率约为 50%（随机猜测），因此模型准确率需要显著高于 50% 才具有实用价值。项目在 CSMD50 数据集上的基准准确率约为 53-55%。

### 马修斯相关系数（MCC）

马修斯相关系数（Matthews Correlation Coefficient，MCC）是一个更稳健的二分类评估指标，考虑了所有四个预测类别的混淆矩阵情况。计算公式为：MCC = (TP×TN - FP×FN) / sqrt((TP+FP)(TP+FN)(TN+FP)(TN+FN))，其中 TP、TN、FP、FN 分别表示真正例、真负例、假正例、假负例。

MCC 的取值范围为 -1 到 +1：+1 表示完美预测；0 表示随机预测；-1 表示完全相反的预测。MCC 相比准确率的优势在于，即使样本不平衡也能给出可靠的评估。在股票预测研究中，MCC 是广泛使用的评估指标。

### 混淆矩阵

混淆矩阵提供了更详细的分类结果视图，显示四种类别的预测情况。矩阵的行列分别表示真实标签和预测标签，每个单元格的值表示对应组合的样本数量。通过混淆矩阵可以分析模型在各类别上的表现差异，发现模型的偏误。例如，如果模型倾向于预测上涨，可以通过混淆矩阵观察到。

混淆矩阵的分析可以帮助识别模型的系统性问题：假正例（FP）高表示模型过于"乐观"，容易错误预测上涨；假负例（FN）高表示模型过于"悲观"，容易错过实际上涨的机会。理解这些偏误对于实际应用决策非常重要。

## 评估流程

### 评估数据准备

模型评估需要准备测试数据集。测试集应包含模型训练期间未见过的数据，以确保评估结果反映模型的泛化能力。项目默认使用时间划分得到的测试集（2024-08-08 至 2024-12-31），这部分数据在训练过程中完全不会被使用。

评估数据的加载方式与训练数据类似，但通常不需要打乱顺序。评估时使用训练好的模型对测试集进行预测，收集预测结果和真实标签用于后续的指标计算。项目提供了 eval_single.py 和 eval_multi.py 脚本分别用于单模态和多模态模型的评估。

### 评估脚本使用

单模态模型评估命令如下：`python eval_single.py --dataset CSMD50 --model LSTM --model_path ./result/CSMD50/model_saved/LSTM.pth --test_price_folder ./dataset/CSMD50/test/price/`。此命令加载训练好的 LSTM 模型，在测试集上进行预测并计算评估指标。

多模态模型评估命令如下：`python eval_multi.py --dataset CSMD50 --model HAN --model_path ./result/CSMD50/model_saved/HAN.pth --test_price_folder ./dataset/CSMD50/test/price/ --test_news_folder ./dataset/CSMD50/test/news_embedding/`。此命令加载训练好的 HAN 模型，在多模态测试数据上进行预测和评估。

### 评估结果解读

评估完成后，脚本会输出各项指标的值。典型的输出包括：测试集准确率（Test ACC）；测试集 MCC；各类别的预测数量（True Positive、True Negative、False Positive、False Negative）。建议将评估结果与基准模型进行对比，判断模型是否有改进。

评估结果的文件保存路径由 `--test_result_save_folder` 参数指定，结果保存为文本文件格式，便于后续分析和比较。建议记录每次评估的配置和结果，建立实验日志以追踪模型改进过程。

## 结果可视化

### 损失曲线

训练过程中的损失曲线是诊断模型行为的重要工具。项目使用 SwanLab 自动记录和可视化损失曲线。训练损失曲线应该呈现下降趋势，验证损失曲线应该先下降后趋于平稳。如果验证损失在某个点开始上升，说明出现过拟合。

损失曲线的分析要点包括：观察损失下降的速度，前期快速下降通常表示模型在学习；观察损失的稳定性，剧烈波动可能需要调整学习率；观察验证损失相对于训练损失的位置，两者差距过大表示过拟合；观察最终收敛的损失值，损失过高可能表示欠拟合。

### 预测分布

模型预测值的分布可以揭示模型的行为特征。理想情况下，模型对正类和负类的预测分布应该有一定的区分度。如果预测值集中在 0.5 附近，说明模型区分能力不强；如果预测值分布偏向极端（接近 0 或 1），说明模型预测较为确定。可以使用直方图或密度图来可视化预测分布。

预测分布的分析可以帮助判断模型是否过度自信。对于过于自信的模型，虽然预测准确但置信度可能不可靠。在实际应用中，可能需要对预测概率进行校准，使其更接近真实的正确概率。

### ROC 曲线

ROC 曲线（Receiver Operating Characteristic curve）展示了在不同阈值下真正例率（TPR）和假正例率（FPR）的关系。曲线越靠近左上角，模型性能越好。AUC（Area Under the Curve）是 ROC 曲线下的面积，取值范围为 0 到 1，越接近 1 表示模型越好。

ROC 曲线特别适用于评估模型在不同风险偏好下的表现。如果应用场景更看重减少假正例（避免错误买入），可以选择较高的阈值；如果更看重减少假负例（避免错过机会），可以选择较低的阈值。

## 模型比较

### 基准模型

项目提供了多个基准模型的预训练权重和评估结果，包括：LSTM 标准长短期记忆网络；BiLSTM 双向长短期记忆网络；ALSTM 注意力增强 LSTM；Adv-LSTM 对抗训练 LSTM；SCINet 采样交叉交互网络；DTML 动态时间记忆网络；StockNet 股票预测网络；HAN 层级注意力网络；PEN 预训练金融模型。了解这些基准模型的性能有助于判断自定义模型是否有所改进。

### 比较方法

进行模型比较时，应确保以下条件：使用相同的训练集、验证集和测试集划分；使用相同的数据预处理方式；使用相同的评估指标；进行多次实验取平均结果以减少随机性。模型比较的结果应记录在实验日志中，包括模型名称、配置参数、性能指标和训练时间等信息。

### 统计显著性

当比较两个模型的性能差异时，需要考虑差异是否具有统计显著性。常用的方法是进行假设检验（如 t-test），判断观察到的性能差异是否由随机波动引起而非真实的性能差异。对于小规模的差异，即使一个模型在数值上更好，也可能不具有统计显著性。

## 评估最佳实践

### 避免数据泄露

评估模型时必须确保测试数据与训练数据完全分离。任何形式的数据泄露都会导致评估结果偏高，无法反映模型在真实场景中的表现。项目通过时间顺序划分数据集，确保每个测试样本的交易日期晚于所有训练样本的交易日期，从根本上避免了泄露风险。

数据泄露的常见来源包括：使用全局标准化参数时在测试集上重新计算均值和标准差；在特征工程中使用未来信息；在数据预处理时混合训练集和测试集。项目的数据加载器已正确处理这些问题，用户无需额外担心。

### 多时间窗口评估

建议在不同的时间窗口上评估模型，以验证模型的稳健性。例如，可以在完整测试集上评估，也可以在按月份或按季度划分的子集上评估。如果模型在所有时间窗口上都表现一致，说明模型具有较好的稳健性；如果表现波动较大，说明模型可能对特定时间段过度拟合。

### 结果记录与复现

每次评估都应详细记录实验配置，包括：模型类型和版本；训练集、验证集、测试集的时间范围；所有超参数设置；随机种子；软件环境版本；评估结果和观察发现。这些记录对于复现实验、排查问题和比较不同实验至关重要。

## 评估后的操作

### 模型选择

根据评估结果选择最佳模型时，应综合考虑多个因素：主要指标（如 ACC、MCC）的性能；结果的稳定性（多次实验的方差）；计算效率（训练和推理时间）；模型复杂度（参数量、内存占用）。没有完美的模型，需要根据具体应用场景权衡这些因素。

### 模型保存

选定最佳模型后，应妥善保存模型权重和相关文件。保存内容包括：模型权重文件（.pth）；模型配置参数；评估结果摘要；数据版本信息。良好的保存习惯有助于后续的模型部署和复现研究。

### 进一步优化

评估结果可以指导进一步的模型优化：如果准确率低于预期，可以尝试更复杂的模型或调整超参数；如果过拟合严重，可以增加正则化或简化模型；如果某些类别的预测效果差，可以考虑类别权重调整或数据增强。
