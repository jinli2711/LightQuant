# 单模态模型

本章节详细介绍 LightQuant 项目中仅使用价格数据进行预测的单模态模型，包括模型原理、架构设计和应用场景。

## LSTM

### 模型原理

长短期记忆网络（Long Short-Term Memory，LSTM）是一种特殊的循环神经网络，能够学习长期依赖关系。LSTM 由 Hochreiter 和 Schmidhuber 于 1997 年提出，通过引入门控机制解决了传统 RNN 的梯度消失问题。

LSTM 的核心是细胞状态（cell state），它像一条传送带贯穿整个网络，只有少量线性交互，信息很容易在上面保持流动。LSTM 通过三个门来控制细胞状态：遗忘门决定哪些信息从细胞状态中丢弃；输入门决定哪些新信息存储到细胞状态；输出门决定输出什么信息。

### 架构设计

LightQuant 中的 LSTM 实现包含以下组件：输入层接收形状为 (batch, seq_len, input_size) 的张量；LSTM 层使用 hidden_size 个隐藏单元和 num_layers 层；BatchNorm1d 层对 LSTM 输出进行批归一化；全连接层将隐藏状态映射到输出空间；Sigmoid 激活层将输出限制在 (0, 1) 区间。

### 应用场景

LSTM 适合以下应用场景：作为其他模型的基线对比；数据量较小的实验（参数少，不易过拟合）；需要快速训练和迭代的研究；实时预测场景（推理速度快）。

### 使用示例

```bash
python run.py --dataset CSMD50 --use_news False --model lstm --epochs 100 --hidden_size 128 --layers 2 --batch_size 64 --lr 3e-4
```

### 参数配置建议

input_size 默认 5，对应 OHLC 五个价格特征。hidden_size 建议 64-256，根据数据量调整。layers 建议 1-3，层数过多容易过拟合。dropout 建议 0.1-0.3，用于正则化。look_back_window 建议 5-14 天。

## BiLSTM

### 模型原理

双向 LSTM（Bidirectional LSTM，BiLSTM）由两个方向的 LSTM 组成，一个处理正向序列，一个处理反向序列。BiLSTM 能够同时利用过去和未来的上下文信息，在许多序列标注任务中表现优异。

BiLSTM 的结构是前向 LSTM 和后向 LSTM 的拼接，它们的隐藏状态在每个时间步合并，形成更丰富的表示。这种设计特别适合那些需要完整序列信息的任务。

### 架构设计

LightQuant 中的 BiLSTM 实现包含以下组件：双向 LSTM 层，每个方向有 hidden_size 个单元，拼接后输出维度为 2×hidden_size；BatchNorm1d 层处理双倍维度的输出；全连接层和 Sigmoid 激活层。

### 应用场景

BiLSTM 适合以下应用场景：需要利用完整上下文信息的预测任务；与单向 LSTM 对比实验；中等规模数据集（参数约为 LSTM 的两倍）。

### 使用示例

```bash
python run.py --dataset CSMD50 --use_news False --model bi_lstm --epochs 100 --hidden_size 128 --layers 2 --batch_size 64 --lr 3e-4
```

### 与 LSTM 的区别

BiLSTM 相比 LSTM 的主要区别：参数数量约为 LSTM 的两倍；训练时间更长；通常能获得更好的性能；推理时需要完整序列（不能用于真正的在线预测）。

## ALSTM

### 模型原理

注意力 LSTM（Attention LSTM，ALSTM）在 LSTM 基础上增加了注意力机制。注意力机制允许模型在做出预测时关注输入序列中最相关的部分，而不是平等对待所有时间步的信息。

ALSTM 的注意力机制计算每个时间步的注意力权重，然后对 LSTM 输出进行加权求和，得到一个上下文向量。这个上下文向量包含了与当前预测最相关的信息。

### 架构设计

ALSTM 在 LSTM 基础上增加：注意力权重计算层；加权求和的上下文向量生成；注意力输出与最终隐藏状态的融合。

### 应用场景

ALSTM 适合以下应用场景：需要解释模型关注哪些时间步；输入序列较长，希望关注关键时点；与标准 LSTM 对比注意力机制的效果。

### 使用示例

```bash
python run.py --dataset CSMD50 --use_news False --model alstm --epochs 100 --hidden_size 128 --attention_size 128 --layers 2 --batch_size 64 --lr 3e-4
```

### 注意力可视化

ALSTM 的注意力权重可以可视化，帮助理解模型决策。通过分析注意力权重分布，可以识别模型认为重要的历史时间点。

## Adv-LSTM

### 模型原理

对抗训练 LSTM（Adversarial LSTM，Adv-LSTM）引入了对抗训练技术。对抗训练通过在输入上添加精心构造的对抗扰动来训练模型，使模型对微小扰动更加鲁棒，从而提升泛化能力。

Adv-LSTM 的核心思想是：不仅要拟合训练数据的标签，还要能够抵抗输入的对抗扰动。这种训练方式使模型学习到更鲁棒的特征表示。

### 架构设计

Adv-LSTM 在 LSTM 基础上增加：对抗扰动生成模块；扰动后的前向传播路径；对抗损失计算（与主损失加权结合）。

### 应用场景

Adv-LSTM 适合以下应用场景：对模型鲁棒性有要求的场景；数据中存在噪声或扰动；防止模型对特定模式过度拟合。

### 使用示例

```bash
python run.py --dataset CSMD50 --use_news False --model adv_lstm --epochs 100 --hidden_size 128 --attention_size 128 --epsilon 0.1 --perturbation_size 0.1
```

### 参数说明

epsilon 控制对抗扰动的强度，建议 0.05-0.2。perturbation_size 控制扰动的最大范数。这两个参数需要根据数据特性调整，过大会影响正常学习，过小则失去对抗训练的效果。

## BiGRU

### 模型原理

门控循环单元（Gated Recurrent Unit，GRU）是另一种门控 RNN 结构，相比 LSTM 结构更简单，只有两个门（更新门和重置门）。GRU 在许多任务上能达到与 LSTM 相似的效果，但参数更少，训练速度更快。

BiGRU 是双向 GRU，能够同时利用前向和后向的上下文信息。

### 架构设计

BiGRU 使用 GRU 单元替代 LSTM，其他结构与 BiLSTM 类似。GRU 的参数约为 LSTM 的 2/3，训练速度更快。

### 应用场景

BiGRU 适合以下应用场景：追求训练效率的场景；与 GRU 和 LSTM 进行对比实验；资源受限但需要双向信息的场景。

### 使用示例

```bash
python run.py --dataset CSMD50 --use_news False --model bigru --epochs 100 --hidden_size 128 --layers 2 --batch_size 64 --lr 3e-4
```

## DTML

### 模型原理

动态时间记忆网络（Dynamic Temporal Memory Network，DTML）专门设计用于多股票联合建模。DTML 能够捕捉股票之间的动态关联关系，学习跨股票的依赖模式。

DTML 的核心思想是：通过注意力机制在多个股票之间建立连接，使模型能够学习哪些股票之间存在关联，以及这种关联如何随时间变化。

### 架构设计

DTML 的主要组件包括：多股票输入处理，同时接收 n_stocks 只股票的数据；跨股票注意力机制，建模股票间关系；序列编码器处理时间动态；输出层生成每只股票的预测。

### 应用场景

DTML 适合以下应用场景：需要利用股票间关联的预测；投资组合层面的分析；行业或板块联动的建模。

### 使用示例

```bash
python run.py --dataset CSMD50 --use_news False --model dtml --epochs 100 --hidden_size 64 --n_stocks 5 --layers 2 --batch_size 32 --lr 3e-4
```

### 特殊配置

n_stocks 参数指定每次训练的股票数量，建议 3-10。较大的 n_stocks 提供更丰富的跨股票信息，但计算量也更大。回测时需要使用 DTML_Dataset 和相应的回测函数。

## SCINet

### 模型原理

采样交叉交互网络（Sampling and Cross-flow Interaction Network，SCINet）是一种专为时间序列预测设计的模型。SCINet 通过采样和交叉交互机制，能够有效捕捉时间序列中的多尺度模式和复杂依赖关系。

SCINet 的核心创新是采样操作和交叉交互模块。采样操作将序列下采样以捕捉不同时间尺度的模式；交叉交互模块在不同采样序列之间建立信息流动。

### 架构设计

SCINet 的主要组件包括：输入投影层，将原始特征映射到隐藏空间；多个 SCINet 层，每层包含采样和交叉交互；预测头，生成预测结果。

### 应用场景

SCINet 适合以下应用场景：长时间序列预测（序列长度 > 30 天）；需要捕捉多尺度时间模式；追求高精度预测效果。

### 使用示例

```bash
python run.py --dataset CSMD50 --use_news False --model scinet --epochs 100 --seq_len 32 --pred_len 5 --hidden_size 128 --SCINet_Layers 3 --batch_size 32 --lr 3e-4
```

### 特殊配置

seq_len 设置输入序列长度，默认 32。pred_len 设置预测长度，默认 5。SCINet_Layers 设置网络层数，默认 3。输入特征数自动扩展为 8（包含时间特征）。

## 模型对比总结

### 复杂度排序

按模型复杂度从低到高排序：LSTM < BiGRU < BiLSTM < ALSTM < Adv-LSTM < DTML < SCINet < StockNet < HAN < PEN。

### 性能排序

按典型预测性能排序（仅供参考）：LSTM ≈ BiGRU < BiLSTM < ALSTM ≈ Adv-LSTM < DTML < SCINet < StockNet < HAN < PEN。

### 选择建议

快速实验和基线：选择 LSTM 或 BiGRU。需要注意力机制：选择 ALSTM。需要鲁棒性：选择 Adv-LSTM。多股票联合建模：选择 DTML。长时间序列：选择 SCINet。

### 资源消耗对比

按资源消耗从低到高排序：LSTM ≈ BiGRU < BiLSTM < ALSTM < Adv-LSTM < DTML < SCINet < StockNet < HAN < PEN。

### 训练时间对比

按单轮训练时间从低到高排序：LSTM ≈ BiGRU < BiLSTM < ALSTM ≈ Adv-LSTM < DTML < SCINet < StockNet < HAN < PEN。
