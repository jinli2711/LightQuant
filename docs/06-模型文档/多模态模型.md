# 多模态模型

本章节详细介绍 LightQuant 项目中同时使用价格数据和新闻文本数据进行预测的多模态模型，包括模型原理、架构设计和应用场景。

## 多模态模型概述

多模态模型通过整合价格数据和新闻文本数据，能够同时捕捉市场中的价格模式和消息面影响。相比单模态模型，多模态模型通常能获得更好的预测效果，因为它们利用了更丰富的信息来源。

### 多模态融合的优势

多模态融合能够带来以下优势：信息互补，价格数据反映市场历史走势，新闻数据反映影响市场的信息；信号增强，新闻中的利好或利空信息可以强化或抵消价格趋势的信号；上下文理解，新闻提供价格变动的背景信息，帮助理解市场行为；噪声过滤，同时考虑多个数据源可以减少单一数据源的噪声影响。

### 挑战与考虑

多模态模型也面临一些挑战：数据准备更复杂，需要同时准备价格数据和新闻数据；训练时间更长，需要处理额外的文本数据；计算资源需求更高；需要处理两个数据源的对齐问题；新闻质量参差不齐，可能引入噪声。

## StockNet

### 模型原理

StockNet 是一个专门为股票预测设计的多模态模型，其核心思想是分别对价格序列和新闻文本进行编码，然后通过融合机制整合两种表示进行预测。

StockNet 的创新点在于：价格编码器使用变分自编码器（VAE）捕捉价格模式；新闻编码器使用注意力机制关注重要新闻；动态融合机制根据上下文自适应地融合两种表示。

### 架构设计

StockNet 的主要组件包括：价格编码器，由多层 LSTM 组成，输出价格序列的表示；新闻编码器，处理每日的新闻序列，使用注意力机制聚合；融合层，将价格表示和新闻表示进行拼接和变换；预测层，生成最终的预测概率。

### 输入格式

价格输入形状：(batch, seq_len, price_features)，seq_len 通常为回看窗口大小，price_features 默认 5。新闻输入形状：(batch, seq_len, news_features)，news_features 为嵌入维度。

### 应用场景

StockNet 适合以下应用场景：中等规模数据集；需要整合价格和技术面信息；对计算资源有一定要求；追求较好的预测效果。

### 使用示例

```bash
python run.py --dataset CSMD50 --use_news True --model StockNet --epochs 100 --hidden_size 128 --batch_size 64 --lr 3e-4
```

### 参数配置

hidden_size 设置隐藏层大小，建议 64-128。batch_size 由于涉及两种数据，建议 32-64。look_back_window 建议 7-14 天。

### 输出格式

输出形状为 (batch, 1)，表示预测的上涨概率。

## HAN

### 模型原理

层级注意力网络（Hierarchical Attention Network，HAN）采用层级注意力机制处理多日多新闻的复杂场景。HAN 的核心思想是：在词语级别和新闻级别分别应用注意力机制，逐层提炼关键信息。

HAN 的层级结构：词语级别，对单条新闻内的词语应用注意力；新闻级别，对单日内的多条新闻应用注意力；日期级别，对多日的信息应用注意力。

### 架构设计

HAN 的主要组件包括：词语级注意力，对嵌入后的词语序列计算注意力权重；新闻级注意力，对单日多条新闻的表示进行聚合；日期级注意力，对多日表示进行加权；价格编码器，独立处理价格序列；多模态融合，拼接价格和新闻表示并预测。

### 输入格式

价格输入形状：(batch, days, price_features)，days 通常等于 look_back_window。新闻输入形状：(batch, days, max_news, embed_dim)，max_news 是每日最大新闻数。

### 应用场景

HAN 适合以下应用场景：需要处理每日多条新闻；新闻数量较多的股票；对可解释性有要求（可以分析各级注意力权重）；追求较好的预测效果。

### 使用示例

```bash
python run.py --dataset CSMD50 --use_news True --model HAN --epochs 100 --hidden_size 128 --attention_size 128 --max_num_tweets_len 20 --max_num_tokens_len 30 --days 7 --batch_size 32 --lr 3e-4
```

### 特殊参数

max_num_tweets_len 设置每日最大新闻数量，默认为 20。max_num_tokens_len 设置新闻文本的最大 token 数量，默认为 30。days 设置回看天数，与 look_back_window 对应。

### 注意力可视化

HAN 的各级注意力权重可以可视化，帮助理解：哪些词语最重要（词语级注意力）；哪些新闻最重要（新闻级注意力）；哪些日期最重要（日期级注意力）。

## PEN

### 模型原理

预训练金融模型（Pretrained Financial Model，PEN）使用预训练的语言模型（如 FinBERT）编码金融新闻文本。FinBERT 是专门在金融语料上预训练的 BERT 模型，能够更好地理解金融领域的专业术语和表达方式。

PEN 的核心优势在于：利用大规模预训练知识；金融领域适应性强；文本编码质量高。

### 架构设计

PEN 的主要组件包括：BERT 编码器，使用预训练的 FinBERT 对新闻文本进行编码；价格编码器，处理价格序列特征；特征融合，将 BERT 输出和价格特征拼接；预测头，生成最终预测。

### 输入格式

价格输入形状：(batch, seq_len, price_features)。新闻输入形状：(batch, days, max_tokens)，需要提供文本 token IDs 而非预计算嵌入。

### 应用场景

PEN 适合以下应用场景：有充足的计算资源（BERT 推理计算量大）；追求最高的多模态预测效果；需要高质量的文本理解；使用金融领域专业术语较多的场景。

### 使用示例

```bash
python run.py --dataset CSMD50 --use_news True --model PEN --epochs 100 --hidden_size 128 --pretrained_model yiyanghkust/finbert-pretrain --freeze_bert True --batch_size 16 --lr 3e-4
```

### 特殊参数

pretrained_model 设置预训练模型名称或路径，默认为 'yiyanghkust/finbert-pretrain'。freeze_bert 设置是否冻结 BERT 参数，默认为 True（不训练 BERT，节省显存）。

### 显存优化

由于 BERT 模型较大，PEN 需要较多显存。优化建议：设置 freeze_bert=True 只训练上层分类器；使用较小的 batch_size；如有多张 GPU，使用数据并行。

## 模型对比

### 架构特点对比

StockNet 采用简单的拼接融合，结构相对简单。HAN 采用层级注意力，能够处理多层级的信息。PEN 使用预训练语言模型，文本理解能力强。

### 资源消耗对比

StockNet 资源消耗中等，是多模态模型的入门选择。HAN 资源消耗较高，需要处理多级注意力计算。PEN 资源消耗最高，BERT 推理需要大量显存。

### 预测性能对比

通常情况下，PEN > HAN > StockNet，但具体效果取决于数据集特性。建议通过实验验证选择最适合的模型。

### 训练时间对比

StockNet 训练时间最短。HAN 训练时间中等。PEN 训练时间最长，特别是全参数微调时。

## 数据准备要点

### 新闻数据质量

多模态模型的效果高度依赖新闻数据质量。注意以下几点：新闻应与对应股票真正相关；新闻时间应准确对应交易日；避免过多重复或相似的新闻；过滤广告、软文等低质量内容。

### 数据对齐

价格数据和新闻数据需要在日期上正确对齐：每条价格数据对应其发布日前的新闻；新闻发布时间和交易日期的对应关系需要正确处理；缺失的新闻日期需要进行适当处理。

### 数据量考虑

多模态模型需要足够的数据来训练：确保有足够的样本数量；每个股票应有足够的新闻覆盖；可以考虑数据增强扩充训练数据。

## 最佳实践

### 训练策略

训练多模态模型时建议：使用预训练词向量减少训练时间；采用渐进式训练策略，先固定文本编码器再联合训练；使用较小的学习率避免破坏预训练权重；注意监控验证集指标防止过拟合。

### 参数调优

多模态模型的参数调优建议：从项目推荐的默认参数开始；重点调优融合层相关参数；注意力机制的参数影响较大；batch_size 需要根据显存调整。

### 模型选择

模型选择建议：资源有限时选择 StockNet；追求性能和可解释性选择 HAN；追求最高性能且资源充足选择 PEN。
