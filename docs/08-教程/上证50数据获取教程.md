# 上证50数据获取教程

## 1. 教程概述

本教程将详细介绍如何获取上证50指数成分股的股票列表、日线价格数据和对应的新闻数据。通过本教程，您将学会使用LightQuant项目中的数据收集工具，为后续的量化分析和模型训练准备数据。

## 2. 准备工作

### 2.1 安装依赖库

确保已安装以下依赖库：

```bash
pip install pandas baostock selenium beautifulsoup4 requests
```

### 2.2 配置Chrome浏览器

1. 下载并安装Chrome浏览器
2. 确保ChromeDriver与Chrome浏览器版本匹配
3. 将ChromeDriver添加到系统PATH中 

### 2.3 项目目录结构

确保项目目录结构如下：

```
LightQuant/
├── dataset_construction/
│   ├── news_scraper.py          # 新闻数据抓取脚本
│   └── price_data_collection.py # 价格数据收集脚本
├── docs/                        # 文档目录
├── llm_factor/
│   └── CSMD50.csv               # 上证50成分股列表
└── data/                        # 数据输出目录
    └── A_Stocks_Data/           # 价格数据输出目录
```

## 3. 步骤1：获取上证50股票列表

### 3.1 查看上证50成分股文件

上证50成分股列表已存储在 `llm_factor/CSMD50.csv` 文件中，您可以直接查看或使用该文件：

```bash
cat llm_factor/CSMD50.csv
```

### 3.2 文件格式说明

该文件包含以下列：

| 字段名 | 描述 |
|-------|------|
| updateDate | 更新日期 |
| code | 股票代码（包含交易所前缀，如 sh.600000） |
| code_name | 股票名称 |

### 3.3 自定义获取股票列表

如果您需要自定义获取股票列表，可以修改 `price_data_collection.py` 中的 `get_sz50_stocks()` 方法，或使用以下代码获取：

```python
import pandas as pd

# 读取上证50成分股列表
sz50_stocks = pd.read_csv('llm_factor/CSMD50.csv', encoding='utf-8')

# 查看股票列表
print(sz50_stocks)

# 提取股票代码和名称
stock_codes = sz50_stocks['code'].tolist()
stock_names = sz50_stocks['code_name'].tolist()
```

## 4. 步骤2：获取日线价格数据

### 4.1 修改配置参数

打开 `dataset_construction/price_data_collection.py` 文件，修改以下参数：

```python
if __name__ == "__main__":
    start_date = "2019-01-01"  # 开始日期
    end_date = "2024-12-18"    # 结束日期
    current_date = "2024-12-19"  # 当前日期
    
    data_collector = A_Stocks_DataCollection(start_date, end_date, current_date)
    data_collector.get_data()
    # data_collector.get_new_data()  # 仅获取指定日期的数据
```

### 4.2 运行价格数据收集脚本

在 `dataset_construction` 目录下运行：

```bash
python price_data_collection.py
```

### 4.3 数据输出位置

价格数据将保存到 `../data/A_Stocks_Data/` 目录下，每个股票一个CSV文件，文件名格式为 `{code}.csv`。

### 4.4 价格数据字段说明

| 字段名 | 描述 |
|-------|------|
| date | 日期 |
| code | 股票代码 |
| open | 开盘价 |
| high | 最高价 |
| low | 最低价 |
| close | 收盘价 |
| preclose | 前收盘价 |
| volume | 成交量 |
| amount | 成交额 |
| adjustflag | 复权状态 |
| turn | 换手率 |
| tradestatus | 交易状态 |
| pctChg | 涨跌幅 |
| peTTM | 动态市盈率 |
| pbMRQ | 市净率 |
| psTTM | 市销率 |
| pcfNcfTTM | 市现率 |
| isST | 是否 ST |
| code_name | 股票名称 |
| industry | 行业 |
| industryClassification | 行业分类 |

## 5. 步骤3：获取新闻数据

### 5.1 准备股票列表文件

新闻爬虫脚本需要一个包含股票代码和名称的CSV文件 `Corrected_CSV_File_Last_Date_Summary.csv`，您可以从上证50成分股列表生成：

```python
import pandas as pd

# 读取上证50成分股列表
sz50_stocks = pd.read_csv('../llm_factor/CSMD50.csv', encoding='utf-8')

# 重命名列以匹配新闻爬虫脚本的要求
sz50_stocks = sz50_stocks.rename(columns={'code': 'code', 'code_name': 'code_name'})

# 保存为新闻爬虫脚本所需的文件
sz50_stocks[['code', 'code_name']].to_csv('Corrected_CSV_File_Last_Date_Summary.csv', index=False, encoding='utf-8')
```

### 5.2 运行新闻爬虫脚本

在 `dataset_construction` 目录下运行：

```bash
python news_scraper.py
```

### 5.3 数据输出位置

- **新闻链接**：保存到 `../news_link/{ticker}.json`
- **新闻内容**：保存到 `../news/{ticker}.json`（成功）或 `../news/{ticker}_failed.json`（失败）

### 5.4 新闻数据格式

```json
[
    {
        "time": "发布时间",
        "title": "新闻标题",
        "content": "新闻正文",
        "link": "原始新闻链接"
    }
]
```

## 6. 数据整合与使用

### 6.1 读取价格数据

```python
import pandas as pd

# 读取单个股票的价格数据
stock_code = 'sh.600000'
price_data = pd.read_csv(f'../data/A_Stocks_Data/{stock_code}.csv', encoding='utf-8-sig')

# 查看数据
print(price_data.head())
```

### 6.2 读取新闻数据

```python
import json

# 读取单个股票的新闻数据
stock_code = 'sh.600000'
with open(f'../news/{stock_code}.json', 'r', encoding='utf-8') as f:
    news_data = json.load(f)

# 查看新闻数量
print(f"共有 {len(news_data)} 条新闻")

# 查看第一条新闻
print(news_data[0])
```

### 6.3 数据整合示例

```python
import pandas as pd
import json
from datetime import datetime

# 读取价格数据
stock_code = 'sh.600000'
price_data = pd.read_csv(f'../data/A_Stocks_Data/{stock_code}.csv', encoding='utf-8-sig')

# 读取新闻数据
with open(f'../news/{stock_code}.json', 'r', encoding='utf-8') as f:
    news_data = json.load(f)

# 将新闻数据转换为DataFrame
news_df = pd.DataFrame(news_data)

# 转换新闻时间格式
news_df['time'] = pd.to_datetime(news_df['time'])

# 提取日期部分
news_df['date'] = news_df['time'].dt.date

# 统计每日新闻数量
daily_news_count = news_df.groupby('date').size().reset_index(name='news_count')

# 将日期转换为字符串格式
price_data['date'] = pd.to_datetime(price_data['date']).dt.date
daily_news_count['date'] = pd.to_datetime(daily_news_count['date']).dt.date

# 合并价格数据和新闻数量
merged_data = pd.merge(price_data, daily_news_count, on='date', how='left')

# 填充缺失值
merged_data['news_count'] = merged_data['news_count'].fillna(0)

# 查看合并后的数据
print(merged_data.head())
```

## 7. 常见问题与解决方案

### 7.1 价格数据获取失败

- **问题**：`price_data_collection.py` 运行失败
- **解决方案**：
  1. 检查网络连接
  2. 确保Baostock API服务正常
  3. 检查日期格式是否正确
  4. 确保输出目录 `../data/A_Stocks_Data/` 存在

### 7.2 新闻数据获取失败

- **问题**：`news_scraper.py` 运行失败
- **解决方案**：
  1. 检查Chrome浏览器和ChromeDriver版本是否匹配
  2. 确保 `Corrected_CSV_File_Last_Date_Summary.csv` 文件存在且格式正确
  3. 检查网络连接
  4. 网站可能有反爬机制，建议降低请求频率

### 7.3 股票列表文件乱码

- **问题**：`CSMD50.csv` 文件中的股票名称显示乱码
- **解决方案**：使用正确的编码格式打开文件，建议使用 `utf-8` 或 `gbk` 编码

### 7.4 编码错误（UnicodeDecodeError）

- **问题**：运行脚本时出现 `UnicodeDecodeError: 'utf-8' codec can't decode byte 0xd6 in position 0: invalid continuation byte`
- **解决方案**：修改 `llm_factor\CSMD50.csv`的编码为`utf-8`

### 7.5 数据量不足

- **问题**：获取的价格数据或新闻数据量不足
- **解决方案**：
  1. 检查日期范围设置是否正确
  2. 确保股票代码格式正确（包含交易所前缀，如 sh.600000）
  3. 对于新闻数据，某些股票可能确实没有足够的新闻报道

## 8. 扩展建议

1. **自动化数据更新**：可以设置定时任务，定期更新数据
2. **多线程/异步抓取**：修改代码以支持多线程或异步抓取，提高数据获取效率
3. **数据质量检查**：添加数据质量检查功能，确保获取的数据完整可靠
4. **数据预处理**：对获取的数据进行清洗和预处理，便于后续分析和建模
5. **支持更多指数**：修改代码以支持获取其他指数成分股的数据，如沪深300、中证500等

## 9. 总结

通过本教程，您已经学会了如何获取上证50指数成分股的股票列表、日线价格数据和对应的新闻数据。这些数据是量化分析和模型训练的基础，您可以根据自己的需求进一步处理和分析这些数据。

如果您在使用过程中遇到任何问题，可以参考本教程的常见问题部分，或查看项目中的其他文档。祝您使用愉快！